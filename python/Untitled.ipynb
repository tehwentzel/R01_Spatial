{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bae05ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from Constants import Const\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "from pointcloud_utils import *\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "175b9ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(Const.data_dir+'processed_dicoms.p','rb') as f:\n",
    "    plist = pickle.load( f ) \n",
    "len(plist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03a99aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vectorize_contours(pdict,rois=None,min_size = 2):\n",
    "#     contours = pdict['contours']\n",
    "#     contours = pcloud_to_concave_hull(contours)\n",
    "#     if rois is None:\n",
    "#         rois = sorted(contours.keys())\n",
    "#     array = []\n",
    "#     for roi in rois:\n",
    "#         contour = contours.get(roi)\n",
    "#         if contour is None or contour.shape[0] < min_size:\n",
    "#             contour = np.zeros((min_size,3))\n",
    "#         array.append(contour)\n",
    "#     #returns a list of variable size pointclouds\n",
    "#     return array\n",
    "        \n",
    "# def downsample_pcloud(xyz,size=2):\n",
    "# #     print(xyz.shape)\n",
    "#     pcd = o3d.geometry.PointCloud()\n",
    "#     pcd.points = o3d.utility.Vector3dVector(xyz)\n",
    "#     pcd2 = pcd.voxel_down_sample(size)\n",
    "#     xyz_new = np.asarray(pcd2.points)\n",
    "# #     print(xyz_new.shape[0],xyz_new.shape[0]/xyz.shape[0],1/size)\n",
    "#     return xyz_new\n",
    "\n",
    "# def resize_pcloud(xyz, npoints = 2000,jitter=.1):\n",
    "#     #voxel downsample\n",
    "#     if xyz.shape[0] < 2:\n",
    "#         return np.zeros((npoints,3))\n",
    "#     correction = 0\n",
    "#     prev_size = xyz.shape[0]\n",
    "#     while xyz.shape[0] > npoints:\n",
    "#         xyz = downsample_pcloud(xyz,(xyz.shape[0]/npoints)+correction)\n",
    "#         if xyz.shape[0] >= prev_size:\n",
    "#             correction += .1\n",
    "#         prev_size = xyz.shape[0]\n",
    "# #         print('---')\n",
    "# #     print(xyz.shape)\n",
    "#     #padd pointcloud with the centroid of the point + some jitter\n",
    "#     diff = npoints - xyz.shape[0]\n",
    "#     pad = np.zeros((diff,3))\n",
    "#     pad[:] = cloud_centroid(xyz)\n",
    "#     if jitter:\n",
    "#         pad = pad + (np.random.random(pad.shape) - .5)*jitter\n",
    "#     new_points = np.vstack([xyz,pad])\n",
    "# #     print('new points',new_points.shape)\n",
    "# #     print('------------------------')\n",
    "#     return new_points\n",
    "\n",
    "\n",
    "# def vectorize_plist_pclouds(plist,rois=None,size=1000):\n",
    "#     #how to make htis a uniform shape\n",
    "#     arrays = []\n",
    "#     if rois is None:\n",
    "#         rois = list(plist[0]['roi_mask_map'].values())\n",
    "#     for patient in plist:\n",
    "#         arr = vectorize_contours(patient,rois=rois)\n",
    "#         print(arr[0].shape)\n",
    "#         if size is not None and size > 0:\n",
    "#             arr = [resize_pcloud(a,size) for a in arr]\n",
    "#             arr = np.stack(arr)\n",
    "#         arrays.append(arr)\n",
    "#     if size is not None and size > 0:\n",
    "#         arrays = np.stack(arrays)\n",
    "#     return arrays\n",
    "    \n",
    "# test = vectorize_plist_pclouds(plist[2:])\n",
    "# test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a8311ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib as mpl\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "# def plot_pcloud_array(array,figsize=(8,8)):\n",
    "#     fig = plt.figure(figsize=figsize,facecolor='w')\n",
    "#     ax = fig.add_subplot(projection='3d')\n",
    "#     ax.set_facecolor('w')\n",
    "#     ax.grid(False)\n",
    "#     colors = mpl.colormaps['tab20']\n",
    "#     assert(array.ndim == 3)\n",
    "#     assert(array.shape[-1] == 3)\n",
    "#     for pointcloud in array:\n",
    "#         size = .2\n",
    "#         ax.scatter(pointcloud[:,0],pointcloud[:,1],pointcloud[:,2],s=size,color='grey')\n",
    "#     return ax\n",
    "\n",
    "# def plotly_pcloud_array(array):\n",
    "#     data = []\n",
    "#     assert(array.ndim == 3)\n",
    "#     assert(array.shape[-1] == 3)\n",
    "#     for points in array:\n",
    "#         if points.shape[0] < 5:\n",
    "#             continue\n",
    "#         color='gray'\n",
    "#         subplot = go.Scatter3d(\n",
    "#             x=points[:,0], y=points[:,1], z=points[:,2], \n",
    "#             mode='markers',\n",
    "#             marker=dict(size=1.5,color=color)\n",
    "#         )\n",
    "#         data.append(subplot)\n",
    "#     fig = go.Figure(\n",
    "#         data = data,\n",
    "#         layout=dict(\n",
    "#             scene=dict(\n",
    "#                 xaxis=dict(visible=False),\n",
    "#                 yaxis=dict(visible=False),\n",
    "#                 zaxis=dict(visible=False)\n",
    "#             )\n",
    "#         )\n",
    "#     )\n",
    "#     return fig\n",
    "\n",
    "# plotly_pcloud_array(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fcde49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3001cc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code I tried to get convolutional network to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37b14ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 1, 104, 56, 56), (5, 104, 56, 56)]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 17 18 19 20 21]\n"
     ]
    }
   ],
   "source": [
    "def get_array(patient):\n",
    "    #extracts an array of image slices with contours\n",
    "    values = patient['ArrayDicom']\n",
    "    mask = patient['mask']\n",
    "    \n",
    "    #make stuff that isn't a contour 0\n",
    "    good_values = values * (mask > 0)\n",
    "    \n",
    "    #trim slices with no contours\n",
    "    not_empty = good_values.sum(axis=1).sum(axis=1) > 0.01\n",
    "    first_true = -1\n",
    "    last_true = -1\n",
    "    for pos,boolean in enumerate(not_empty):\n",
    "        if boolean:\n",
    "            if first_true < 0:\n",
    "                first_true = pos\n",
    "            else:\n",
    "                last_true = max(last_true,pos)\n",
    "    return good_values[first_true:last_true+1], mask[first_true:last_true]\n",
    "\n",
    "def pad_height(array3d,height,fill=0):\n",
    "    shape = array3d.shape[1:]\n",
    "    npads=height- array3d.shape[0] \n",
    "    assert(npads >= 0)\n",
    "    if npads == 0:\n",
    "        return array3d\n",
    "    \n",
    "    pad = np.full((npads,shape[0],shape[1]),fill)\n",
    "    return np.vstack([array3d,pad])\n",
    "\n",
    "def arrays_from_plist(plist, pad=0,scale= True, add_channel=False,square=False,img_size=56):\n",
    "   #returns  arrays of image vlaues n_patients x n_images x width x height\n",
    "    # x is input images values, y is the class labels\n",
    "    #img_size will resize images. uses linear for x and nearest neighbors when estimating roi membership so edges might be a bit wonky\n",
    "    x,y = zip(*[get_array(p) for p in plist])\n",
    "    x = list(x)\n",
    "    y = list(y)\n",
    "    if square:\n",
    "        max_height = img_size\n",
    "    else:\n",
    "        max_height = np.max([xx.shape[0] for xx in x ])\n",
    "    x = [pad_height(xx,max_height,pad) for xx in x]\n",
    "    y =  [pad_height(yy,max_height,pad) for yy in y]\n",
    "    #make it square\n",
    "    #when resizeing y (organ index labels), i thing inter_nearest uses a nearest neighbors approach so it returns ints also\n",
    "    if img_size is not None:\n",
    "        x = [np.stack([cv2.resize(xxx, (img_size,img_size)) for xxx in xx]) for xx in x]\n",
    "        y = [np.stack([cv2.resize(yyy, (img_size,img_size),interpolation=cv2.INTER_NEAREST) for yyy in yy]) for yy in y]\n",
    "    x = np.stack(x)\n",
    "    y = np.stack(y)\n",
    "    #scale to be 0-1\n",
    "    if scale:\n",
    "        x = (x-x.min())/(x.max() - x.min())\n",
    "    #adds a dimension so  htat the model fits for a 3d convolution for pytorch\n",
    "    if add_channel:\n",
    "        s = x.shape\n",
    "        x = x.reshape((s[0],1,s[1],s[2],s[3]))\n",
    "    return x,y\n",
    "\n",
    "test = arrays_from_plist(plist,add_channel=True)\n",
    "print([t.shape for t in test])\n",
    "print(np.unique(test[1]))\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78feae89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(plist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29c9d6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evl/andrew/anaconda3/envs/Qubbed/lib/python3.7/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 104, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class C3Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_dims = [16,32,64],\n",
    "                 fc_dims=[128], #last fc_dim will be \n",
    "                 conv_kernel_size=3,\n",
    "                 channels=1,\n",
    "                ):\n",
    "\n",
    "        super(C3Encoder, self).__init__()\n",
    "        groups = []\n",
    "        curr_in = channels\n",
    "        for hidden_dim in hidden_dims:\n",
    "            conv = self.make_conv_group(curr_in,hidden_dim,conv_kernel_size)\n",
    "            curr_in = hidden_dim\n",
    "            groups.append(conv)\n",
    "        self.groups= nn.ModuleList(groups)\n",
    "        \n",
    "        self.init_fc_layers(fc_dims)\n",
    "        \n",
    "        \n",
    "    def make_conv_group(self,curr_in, curr_out,kernel_size):\n",
    "        conv = nn.Sequential(\n",
    "            nn.Conv3d(curr_in,curr_out,kernel_size=kernel_size,padding=1),\n",
    "            nn.BatchNorm3d(curr_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(1, 2, 2)),\n",
    "        )\n",
    "        return conv\n",
    "        \n",
    "\n",
    "    def init_fc_layers(self,fc_dims):\n",
    "        fc_layers = []\n",
    "        for fc_dim in fc_dims:\n",
    "            group = self.make_linear_group(fc_dim)\n",
    "            fc_layers.append(group)\n",
    "            \n",
    "        self.fc_layers = nn.ModuleList(fc_layers)\n",
    "        self.output_size = fc_dims[-1]\n",
    "        \n",
    "    def make_linear_group(self, curr_out,**kwargs):\n",
    "        return nn.LazyLinear(curr_out,**kwargs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         x = torch.from_numpy(x).float().cuda()\n",
    "        for group in self.groups:\n",
    "            x = group(x)\n",
    "        out = x.view(x.size(0), -1)\n",
    "        for layer in self.fc_layers:\n",
    "            out = layer(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "    \n",
    "class C3Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                fc_dims=[128,128],\n",
    "                 image_output_size = [104,128,128],\n",
    "                 num_classes = 10,\n",
    "                ):\n",
    "        super(C3Decoder,self).__init__()\n",
    "        \n",
    "        self.init_fc_layers(fc_dims)\n",
    "        #add in a hidden layer with the right size so we end up with enough outputs \n",
    "        #this will just be an image unraveled\n",
    "        output_shape = np.prod(image_output_size)\n",
    "        self.output_shape = image_output_size\n",
    "        #so I can use it as the equivalent of the channels section (this is a bad idea)\n",
    "        \n",
    "        self.num_classes = num_classes \n",
    "        self.final_linear = nn.Linear(fc_dims[-1],output_shape)\n",
    "        self.orient = lambda x: x.view(x.shape[0],1,-1)\n",
    "        self.conv1d = nn.Conv1d(1,num_classes,kernel_size=1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.out_shape = image_output_size\n",
    "    \n",
    "    def init_fc_layers(self,fc_dims):\n",
    "        fc_layers = []\n",
    "        for fc_dim in fc_dims:\n",
    "            group = self.make_linear_group(fc_dim)\n",
    "            fc_layers.append(group)\n",
    "            \n",
    "        self.fc_layers = nn.ModuleList(fc_layers)\n",
    "        self.output_size = fc_dims[-1]\n",
    "        \n",
    "    def make_linear_group(self, curr_out,**kwargs):\n",
    "        return nn.LazyLinear(curr_out,**kwargs)\n",
    "    \n",
    "    def forward(self,x):\n",
    "#         print('decode start',x.shape)\n",
    "        for layer in self.fc_layers:\n",
    "            x = layer(x)\n",
    "#         print('after linear',x.shape)\n",
    "        x = self.final_linear(x)\n",
    "#         print('fl',x.shape)\n",
    "        x = self.orient(x)\n",
    "#         print('orient', x.shape)\n",
    "        x = self.conv1d(x)\n",
    "#         print('conv1d',x.shape)\n",
    "        x = self.softmax(x)\n",
    "#         print('softmax',x.shape)\n",
    "        ##this was if I want to ouptut a single class\n",
    "#         x = torch.argmax(x,dim=1)\n",
    "#         return x.view(x.shape[0],self.out_shape[0],self.out_shape[1],self.out_shape[2])\n",
    "        #returnx ndim x nclasses x nimages x width x height so I can use crossentropy loss\n",
    "        return x.view(x.shape[0],-1,self.out_shape[0],self.out_shape[1],self.out_shape[2])\n",
    "\n",
    "class C3AutoEncoder(nn.Module):\n",
    "    #currently this is a cnn encoder + linear classifier that predicts if a pixel belongs to a part of an roi\n",
    "    #you need to specify the number of rois\n",
    "    def __init__(self,\n",
    "                 example_data, #example imput data, takes last 3 rows to get output image size\n",
    "                 encoder=None,\n",
    "                 decoder=None,\n",
    "                 num_classes=10,\n",
    "                ):\n",
    "        super(C3AutoEncoder,self).__init__()\n",
    "        if encoder is None:\n",
    "            encoder = C3Encoder()\n",
    "            \n",
    "        while example_data.ndim > 3:\n",
    "            example_data = example_data[0]\n",
    "        if decoder is None:\n",
    "            decoder = C3Decoder(image_output_size=example_data.shape,num_classes=num_classes)\n",
    "            \n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "        self.output_shape = example_data.shape\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "testx,testy = arrays_from_plist(plist,add_channel=True,img_size=56)\n",
    "test = C3AutoEncoder(testx[0]).cuda()\n",
    "test\n",
    "print(test( torch.from_numpy(testx[1:3]).float().cuda() ).shape)\n",
    "del test, testx, testy\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4e54442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evl/andrew/anaconda3/envs/Qubbed/lib/python3.7/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss 9.180432796478271 val loss 5.8978331089019775 val acc 0.00021922832820564508 remaining_tries 10\n",
      "epoch 1 train loss 9.197551012039185 val loss 5.918818712234497 val acc 0.00015330652240663767 remaining_tries 9\n",
      "epoch 2 train loss 9.009192705154419 val loss 5.836611032485962 val acc 0.00020849686552537605 remaining_tries 10\n",
      "epoch 3 train loss 8.780360460281372 val loss 5.80911922454834 val acc 0.0001901000869111158 remaining_tries 10\n",
      "epoch 4 train loss 8.727834224700928 val loss 5.77264142036438 val acc 0.00017783555813366547 remaining_tries 10\n",
      "epoch 5 train loss 8.630842208862305 val loss 5.700827598571777 val acc 0.00019316621182952076 remaining_tries 10\n",
      "epoch 6 train loss 8.600394487380981 val loss 5.6377198696136475 val acc 0.00019163314573233947 remaining_tries 10\n",
      "epoch 7 train loss 8.472360849380493 val loss 5.601292133331299 val acc 0.00020696379942819476 remaining_tries 10\n",
      "epoch 8 train loss 8.374368667602539 val loss 5.499507427215576 val acc 0.0015529950032941997 remaining_tries 10\n",
      "epoch 9 train loss 8.206745624542236 val loss 5.398188352584839 val acc 0.0005840978410560638 remaining_tries 10\n",
      "epoch 10 train loss 8.025323152542114 val loss 5.309494972229004 val acc 0.004585397895425558 remaining_tries 10\n",
      "epoch 11 train loss 7.885609865188599 val loss 5.180462837219238 val acc 0.0021125638158991933 remaining_tries 10\n",
      "epoch 12 train loss 7.708798885345459 val loss 5.156977415084839 val acc 0.00197305491019506 remaining_tries 10\n",
      "epoch 13 train loss 7.525987386703491 val loss 4.956342935562134 val acc 0.009572458919137716 remaining_tries 10\n",
      "epoch 14 train loss 7.300981760025024 val loss 4.779512643814087 val acc 0.041823551058769226 remaining_tries 10\n",
      "epoch 15 train loss 7.051591634750366 val loss 4.67432713508606 val acc 0.0426897332072258 remaining_tries 10\n",
      "epoch 16 train loss 6.839580774307251 val loss 4.537154912948608 val acc 0.08191320672631264 remaining_tries 10\n",
      "epoch 17 train loss 6.79163932800293 val loss 4.524555444717407 val acc 0.08639282360672951 remaining_tries 10\n",
      "epoch 18 train loss 6.784593105316162 val loss 4.5210113525390625 val acc 0.08195306733250618 remaining_tries 10\n",
      "epoch 19 train loss 6.7777345180511475 val loss 4.5142903327941895 val acc 0.07687248662114143 remaining_tries 10\n",
      "epoch 20 train loss 6.767254590988159 val loss 4.50700044631958 val acc 0.07743972167372704 remaining_tries 10\n",
      "epoch 21 train loss 6.756555557250977 val loss 4.49964165687561 val acc 0.07487490400671959 remaining_tries 10\n",
      "epoch 22 train loss 6.746351003646851 val loss 4.494082689285278 val acc 0.07446097955107689 remaining_tries 10\n",
      "epoch 23 train loss 6.7382097244262695 val loss 4.488534927368164 val acc 0.07174438238143921 remaining_tries 10\n",
      "epoch 24 train loss 6.730104446411133 val loss 4.484282732009888 val acc 0.06841456517577171 remaining_tries 10\n",
      "epoch 25 train loss 6.72379207611084 val loss 4.47978401184082 val acc 0.06540362536907196 remaining_tries 10\n",
      "epoch 26 train loss 6.7177183628082275 val loss 4.4765894412994385 val acc 0.06628360599279404 remaining_tries 10\n",
      "epoch 27 train loss 6.712549448013306 val loss 4.473357677459717 val acc 0.06466009095311165 remaining_tries 10\n",
      "epoch 28 train loss 6.708242177963257 val loss 4.47071385383606 val acc 0.06251993030309677 remaining_tries 10\n",
      "epoch 29 train loss 6.704585075378418 val loss 4.468445301055908 val acc 0.05997810885310173 remaining_tries 10\n",
      "epoch 30 train loss 6.701326847076416 val loss 4.466509580612183 val acc 0.05745008587837219 remaining_tries 10\n",
      "epoch 31 train loss 6.698704242706299 val loss 4.464944362640381 val acc 0.058725593611598015 remaining_tries 10\n",
      "epoch 32 train loss 6.6963958740234375 val loss 4.463547945022583 val acc 0.060182007029652596 remaining_tries 10\n",
      "epoch 33 train loss 6.693842172622681 val loss 4.461889982223511 val acc 0.059717489406466484 remaining_tries 10\n",
      "epoch 34 train loss 6.691667079925537 val loss 4.460623025894165 val acc 0.055887890979647636 remaining_tries 10\n",
      "epoch 35 train loss 6.690113067626953 val loss 4.459693193435669 val acc 0.058281006291508675 remaining_tries 10\n",
      "epoch 36 train loss 6.688509941101074 val loss 4.458756923675537 val acc 0.057361166924238205 remaining_tries 10\n",
      "epoch 37 train loss 6.687351226806641 val loss 4.458083868026733 val acc 0.057650916278362274 remaining_tries 10\n",
      "epoch 38 train loss 6.686275005340576 val loss 4.457314491271973 val acc 0.05748841166496277 remaining_tries 10\n",
      "epoch 39 train loss 6.685224771499634 val loss 4.456813335418701 val acc 0.057330505922436714 remaining_tries 10\n",
      "epoch 40 train loss 6.684367418289185 val loss 4.456161022186279 val acc 0.05735350213944912 remaining_tries 10\n",
      "epoch 41 train loss 6.683534860610962 val loss 4.45574688911438 val acc 0.05704382248222828 remaining_tries 10\n",
      "epoch 42 train loss 6.682884693145752 val loss 4.455280065536499 val acc 0.05720479413866997 remaining_tries 10\n",
      "epoch 43 train loss 6.68238377571106 val loss 4.455055236816406 val acc 0.0570008959621191 remaining_tries 10\n",
      "epoch 44 train loss 6.681995630264282 val loss 4.454937219619751 val acc 0.05332920514047146 remaining_tries 10\n",
      "epoch 45 train loss 6.681628465652466 val loss 4.454712629318237 val acc 0.053056320175528526 remaining_tries 10\n",
      "epoch 46 train loss 6.681330442428589 val loss 4.45442795753479 val acc 0.056576237082481384 remaining_tries 10\n",
      "epoch 47 train loss 6.68107795715332 val loss 4.4542365074157715 val acc 0.05643366277217865 remaining_tries 10\n",
      "epoch 48 train loss 6.680753469467163 val loss 4.454042673110962 val acc 0.05687671899795532 remaining_tries 10\n",
      "epoch 49 train loss 6.680511236190796 val loss 4.453918218612671 val acc 0.05654711090028286 remaining_tries 10\n",
      "epoch 50 train loss 6.680330276489258 val loss 4.453786373138428 val acc 0.05637693963944912 remaining_tries 10\n",
      "epoch 51 train loss 6.680161952972412 val loss 4.453697443008423 val acc 0.0567847341299057 remaining_tries 10\n",
      "epoch 52 train loss 6.680011034011841 val loss 4.4535534381866455 val acc 0.056565506383776665 remaining_tries 10\n",
      "epoch 53 train loss 6.679781436920166 val loss 4.453514575958252 val acc 0.05331694148480892 remaining_tries 10\n",
      "epoch 54 train loss 6.679660320281982 val loss 4.453384876251221 val acc 0.056544044986367226 remaining_tries 10\n",
      "epoch 55 train loss 6.679559946060181 val loss 4.453475713729858 val acc 0.05110779404640198 remaining_tries 9\n",
      "epoch 56 train loss 6.679502248764038 val loss 4.453258037567139 val acc 0.056353943422436714 remaining_tries 10\n",
      "epoch 57 train loss 6.679368257522583 val loss 4.453230142593384 val acc 0.05431343428790569 remaining_tries 10\n",
      "epoch 58 train loss 6.679290771484375 val loss 4.453167676925659 val acc 0.05607645772397518 remaining_tries 10\n",
      "epoch 59 train loss 6.679270029067993 val loss 4.453128337860107 val acc 0.05650571547448635 remaining_tries 10\n",
      "epoch 60 train loss 6.679198980331421 val loss 4.453082799911499 val acc 0.05641679838299751 remaining_tries 10\n",
      "epoch 61 train loss 6.679135322570801 val loss 4.453029632568359 val acc 0.05630948394536972 remaining_tries 10\n",
      "epoch 62 train loss 6.679062128067017 val loss 4.4530110359191895 val acc 0.056478122249245644 remaining_tries 10\n",
      "epoch 63 train loss 6.679019451141357 val loss 4.45297646522522 val acc 0.056452060118317604 remaining_tries 10\n",
      "epoch 64 train loss 6.678987264633179 val loss 4.453080177307129 val acc 0.05168422684073448 remaining_tries 9\n",
      "epoch 65 train loss 6.678971290588379 val loss 4.452956199645996 val acc 0.05640146881341934 remaining_tries 10\n",
      "epoch 66 train loss 6.678947448730469 val loss 4.452926397323608 val acc 0.05647505633533001 remaining_tries 10\n",
      "epoch 67 train loss 6.678908824920654 val loss 4.45291543006897 val acc 0.05650111846625805 remaining_tries 10\n",
      "epoch 68 train loss 6.678887128829956 val loss 4.452923059463501 val acc 0.05376153066754341 remaining_tries 9\n",
      "epoch 69 train loss 6.678837776184082 val loss 4.452928066253662 val acc 0.051774678751826286 remaining_tries 8\n",
      "epoch 70 train loss 6.678782224655151 val loss 4.452845335006714 val acc 0.05396849289536476 remaining_tries 10\n",
      "epoch 71 train loss 6.678716659545898 val loss 4.4528586864471436 val acc 0.05215487815439701 remaining_tries 9\n",
      "epoch 72 train loss 6.678689002990723 val loss 4.452776908874512 val acc 0.05640606768429279 remaining_tries 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 73 train loss 6.678686618804932 val loss 4.452768087387085 val acc 0.055929284542798996 remaining_tries 10\n",
      "epoch 74 train loss 6.678674697875977 val loss 4.452758550643921 val acc 0.056430596858263016 remaining_tries 10\n",
      "epoch 75 train loss 6.678659677505493 val loss 4.4527668952941895 val acc 0.05585569702088833 remaining_tries 9\n",
      "epoch 76 train loss 6.678627967834473 val loss 4.452791452407837 val acc 0.05312224105000496 remaining_tries 8\n",
      "epoch 77 train loss 6.67858624458313 val loss 4.452703475952148 val acc 0.05636467598378658 remaining_tries 10\n",
      "epoch 78 train loss 6.678558588027954 val loss 4.452669143676758 val acc 0.05635547637939453 remaining_tries 10\n",
      "epoch 79 train loss 6.678510427474976 val loss 4.4526567459106445 val acc 0.056338611990213394 remaining_tries 10\n",
      "epoch 80 train loss 6.678497076034546 val loss 4.45264196395874 val acc 0.056301819160580635 remaining_tries 10\n",
      "epoch 81 train loss 6.678468942642212 val loss 4.452619314193726 val acc 0.05633401311933994 remaining_tries 10\n",
      "epoch 82 train loss 6.678451299667358 val loss 4.452660083770752 val acc 0.053599024191498756 remaining_tries 9\n",
      "epoch 83 train loss 6.678423166275024 val loss 4.452643871307373 val acc 0.05332307331264019 remaining_tries 8\n",
      "epoch 84 train loss 6.678406715393066 val loss 4.452564001083374 val acc 0.0558173693716526 remaining_tries 10\n",
      "epoch 85 train loss 6.67834997177124 val loss 4.45257568359375 val acc 0.0551152266561985 remaining_tries 9\n",
      "epoch 86 train loss 6.678317070007324 val loss 4.452531099319458 val acc 0.056298753246665 remaining_tries 10\n",
      "epoch 87 train loss 6.678303003311157 val loss 4.452511548995972 val acc 0.05630948394536972 remaining_tries 10\n",
      "epoch 88 train loss 6.678264617919922 val loss 4.452477216720581 val acc 0.05630028620362282 remaining_tries 10\n",
      "epoch 89 train loss 6.678234100341797 val loss 4.452516317367554 val acc 0.05430423468351364 remaining_tries 9\n",
      "epoch 90 train loss 6.6782143115997314 val loss 4.452507972717285 val acc 0.05446827411651611 remaining_tries 8\n",
      "epoch 91 train loss 6.678210735321045 val loss 4.452455520629883 val acc 0.05626809224486351 remaining_tries 10\n",
      "epoch 92 train loss 6.678200721740723 val loss 4.452449321746826 val acc 0.056306418031454086 remaining_tries 10\n",
      "epoch 93 train loss 6.678185701370239 val loss 4.452441215515137 val acc 0.05626655928790569 remaining_tries 10\n",
      "epoch 94 train loss 6.678164005279541 val loss 4.45242714881897 val acc 0.056291088461875916 remaining_tries 10\n",
      "epoch 95 train loss 6.67815637588501 val loss 4.452464818954468 val acc 0.05515355244278908 remaining_tries 9\n",
      "epoch 96 train loss 6.678105592727661 val loss 4.452367067337036 val acc 0.056260425597429276 remaining_tries 10\n",
      "epoch 97 train loss 6.678069114685059 val loss 4.4523680210113525 val acc 0.05625122785568237 remaining_tries 9\n",
      "epoch 98 train loss 6.678056955337524 val loss 4.452350854873657 val acc 0.05597834102809429 remaining_tries 10\n",
      "epoch 99 train loss 6.677989482879639 val loss 4.452329874038696 val acc 0.05600900389254093 remaining_tries 10\n",
      "epoch 100 train loss 6.677908897399902 val loss 4.45229434967041 val acc 0.054468272253870964 remaining_tries 10\n",
      "epoch 101 train loss 6.6778459548950195 val loss 4.452199220657349 val acc 0.056177640333771706 remaining_tries 10\n",
      "epoch 102 train loss 6.677809000015259 val loss 4.452181339263916 val acc 0.05613778159022331 remaining_tries 10\n",
      "epoch 103 train loss 6.677774667739868 val loss 4.452229022979736 val acc 0.05525013618171215 remaining_tries 9\n",
      "epoch 104 train loss 6.6777503490448 val loss 4.452124118804932 val acc 0.05614391341805458 remaining_tries 10\n",
      "epoch 105 train loss 6.677703619003296 val loss 4.452120065689087 val acc 0.056131649762392044 remaining_tries 10\n",
      "epoch 106 train loss 6.677698373794556 val loss 4.452123165130615 val acc 0.0560825914144516 remaining_tries 9\n",
      "epoch 107 train loss 6.6776652336120605 val loss 4.452123165130615 val acc 0.055795907974243164 remaining_tries 8\n",
      "epoch 108 train loss 6.677659511566162 val loss 4.452088356018066 val acc 0.05610865168273449 remaining_tries 10\n",
      "epoch 109 train loss 6.67764139175415 val loss 4.452070236206055 val acc 0.056131649762392044 remaining_tries 10\n",
      "epoch 110 train loss 6.677611351013184 val loss 4.45206093788147 val acc 0.05611325241625309 remaining_tries 10\n",
      "epoch 111 train loss 6.677599668502808 val loss 4.452097177505493 val acc 0.056053463369607925 remaining_tries 9\n",
      "epoch 112 train loss 6.677591323852539 val loss 4.452051162719727 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 113 train loss 6.677598714828491 val loss 4.452043294906616 val acc 0.05608872324228287 remaining_tries 10\n",
      "epoch 114 train loss 6.677567720413208 val loss 4.452023506164551 val acc 0.056110184639692307 remaining_tries 10\n",
      "epoch 115 train loss 6.6775453090667725 val loss 4.452023029327393 val acc 0.056125516071915627 remaining_tries 10\n",
      "epoch 116 train loss 6.677546739578247 val loss 4.452024698257446 val acc 0.05612858198583126 remaining_tries 9\n",
      "epoch 117 train loss 6.677529811859131 val loss 4.452007293701172 val acc 0.056127049028873444 remaining_tries 10\n",
      "epoch 118 train loss 6.677516222000122 val loss 4.451994180679321 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 119 train loss 6.6775102615356445 val loss 4.4520018100738525 val acc 0.05612858198583126 remaining_tries 9\n",
      "epoch 120 train loss 6.677494049072266 val loss 4.45198917388916 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 121 train loss 6.677479028701782 val loss 4.4519569873809814 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 122 train loss 6.677452325820923 val loss 4.451937437057495 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 123 train loss 6.677412271499634 val loss 4.4519124031066895 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 124 train loss 6.677379369735718 val loss 4.451882362365723 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 125 train loss 6.677325487136841 val loss 4.451870679855347 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 126 train loss 6.677289009094238 val loss 4.451875448226929 val acc 0.05613011494278908 remaining_tries 9\n",
      "epoch 127 train loss 6.677284240722656 val loss 4.451858282089233 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 128 train loss 6.677259922027588 val loss 4.451826810836792 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 129 train loss 6.677250862121582 val loss 4.451817989349365 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 130 train loss 6.677247524261475 val loss 4.451810121536255 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 131 train loss 6.6772239208221436 val loss 4.4518256187438965 val acc 0.05613011494278908 remaining_tries 9\n",
      "epoch 132 train loss 6.677218914031982 val loss 4.451809883117676 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 133 train loss 6.67720627784729 val loss 4.451796770095825 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 134 train loss 6.677201747894287 val loss 4.45179009437561 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 135 train loss 6.67719578742981 val loss 4.451802968978882 val acc 0.05613011494278908 remaining_tries 9\n",
      "epoch 136 train loss 6.677197217941284 val loss 4.451786518096924 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 137 train loss 6.677183628082275 val loss 4.451781988143921 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 138 train loss 6.677182674407959 val loss 4.451795816421509 val acc 0.05613011494278908 remaining_tries 9\n",
      "epoch 139 train loss 6.677178382873535 val loss 4.451779365539551 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 140 train loss 6.677175045013428 val loss 4.451778888702393 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 141 train loss 6.677170753479004 val loss 4.451866388320923 val acc 0.05613011494278908 remaining_tries 9\n",
      "epoch 142 train loss 6.677184343338013 val loss 4.45177149772644 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 143 train loss 6.677175283432007 val loss 4.451775074005127 val acc 0.05613011494278908 remaining_tries 9\n",
      "epoch 144 train loss 6.677170515060425 val loss 4.4517741203308105 val acc 0.05613011494278908 remaining_tries 8\n",
      "epoch 145 train loss 6.677167654037476 val loss 4.451770782470703 val acc 0.05613011494278908 remaining_tries 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 146 train loss 6.677164554595947 val loss 4.451765537261963 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 147 train loss 6.677166700363159 val loss 4.4518609046936035 val acc 0.05613011494278908 remaining_tries 9\n",
      "epoch 148 train loss 6.677158355712891 val loss 4.451874017715454 val acc 0.05613011494278908 remaining_tries 8\n",
      "epoch 149 train loss 6.677156925201416 val loss 4.451761960983276 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 150 train loss 6.6771533489227295 val loss 4.451761484146118 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 151 train loss 6.67715311050415 val loss 4.451767444610596 val acc 0.05613011494278908 remaining_tries 9\n",
      "epoch 152 train loss 6.677152633666992 val loss 4.451763391494751 val acc 0.05613011494278908 remaining_tries 8\n",
      "epoch 153 train loss 6.677151918411255 val loss 4.451760530471802 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 154 train loss 6.677156209945679 val loss 4.45176362991333 val acc 0.05613011494278908 remaining_tries 9\n",
      "epoch 155 train loss 6.677149772644043 val loss 4.451764106750488 val acc 0.05613011494278908 remaining_tries 8\n",
      "epoch 156 train loss 6.677148818969727 val loss 4.451758623123169 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 157 train loss 6.67715048789978 val loss 4.451858043670654 val acc 0.05613011494278908 remaining_tries 9\n",
      "epoch 158 train loss 6.677148342132568 val loss 4.451760530471802 val acc 0.05613011494278908 remaining_tries 8\n",
      "epoch 159 train loss 6.677148103713989 val loss 4.451757431030273 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 160 train loss 6.677147150039673 val loss 4.451758861541748 val acc 0.05613011494278908 remaining_tries 9\n",
      "epoch 161 train loss 6.677147388458252 val loss 4.451759576797485 val acc 0.05613011494278908 remaining_tries 8\n",
      "epoch 162 train loss 6.677145957946777 val loss 4.451758623123169 val acc 0.05613011494278908 remaining_tries 7\n",
      "epoch 163 train loss 6.677147626876831 val loss 4.451756715774536 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 164 train loss 6.67714524269104 val loss 4.451815128326416 val acc 0.05613011494278908 remaining_tries 9\n",
      "epoch 165 train loss 6.67714524269104 val loss 4.451756954193115 val acc 0.05613011494278908 remaining_tries 8\n",
      "epoch 166 train loss 6.677144527435303 val loss 4.451864957809448 val acc 0.05613011494278908 remaining_tries 7\n",
      "epoch 167 train loss 6.6771461963653564 val loss 4.451833009719849 val acc 0.05613011494278908 remaining_tries 6\n",
      "epoch 168 train loss 6.677151679992676 val loss 4.451756715774536 val acc 0.05613011494278908 remaining_tries 5\n",
      "epoch 169 train loss 6.677144289016724 val loss 4.451756000518799 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 170 train loss 6.677143096923828 val loss 4.451754808425903 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 171 train loss 6.677144289016724 val loss 4.451807260513306 val acc 0.05613011494278908 remaining_tries 9\n",
      "epoch 172 train loss 6.677143573760986 val loss 4.4517552852630615 val acc 0.05613011494278908 remaining_tries 8\n",
      "epoch 173 train loss 6.6771440505981445 val loss 4.451755523681641 val acc 0.05613011494278908 remaining_tries 7\n",
      "epoch 174 train loss 6.677145481109619 val loss 4.451821327209473 val acc 0.05613011494278908 remaining_tries 6\n",
      "epoch 175 train loss 6.677142381668091 val loss 4.451817750930786 val acc 0.05613011494278908 remaining_tries 5\n",
      "epoch 176 train loss 6.677142858505249 val loss 4.4517552852630615 val acc 0.05613011494278908 remaining_tries 4\n",
      "epoch 177 train loss 6.677143573760986 val loss 4.451756000518799 val acc 0.05613011494278908 remaining_tries 3\n",
      "epoch 178 train loss 6.677143096923828 val loss 4.4517552852630615 val acc 0.05613011494278908 remaining_tries 2\n",
      "epoch 179 train loss 6.677142381668091 val loss 4.451785564422607 val acc 0.05613011494278908 remaining_tries 1\n",
      "epoch 180 train loss 6.677141189575195 val loss 4.451753616333008 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 181 train loss 6.677140712738037 val loss 4.451753854751587 val acc 0.05613011494278908 remaining_tries 9\n",
      "epoch 182 train loss 6.677140474319458 val loss 4.451755523681641 val acc 0.05613011494278908 remaining_tries 8\n",
      "epoch 183 train loss 6.677140474319458 val loss 4.451812505722046 val acc 0.05613011494278908 remaining_tries 7\n",
      "epoch 184 train loss 6.677142143249512 val loss 4.451772451400757 val acc 0.05613011494278908 remaining_tries 6\n",
      "epoch 185 train loss 6.6771399974823 val loss 4.451825380325317 val acc 0.05613011494278908 remaining_tries 5\n",
      "epoch 186 train loss 6.6771399974823 val loss 4.451819658279419 val acc 0.05613011494278908 remaining_tries 4\n",
      "epoch 187 train loss 6.677141904830933 val loss 4.451758861541748 val acc 0.05613011494278908 remaining_tries 3\n",
      "epoch 188 train loss 6.677140712738037 val loss 4.451829433441162 val acc 0.05613011494278908 remaining_tries 2\n",
      "epoch 189 train loss 6.677139520645142 val loss 4.45175313949585 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 190 train loss 6.67714262008667 val loss 4.451822519302368 val acc 0.05613011494278908 remaining_tries 9\n",
      "epoch 191 train loss 6.6771392822265625 val loss 4.451752662658691 val acc 0.05613011494278908 remaining_tries 10\n",
      "epoch 192 train loss 6.677146673202515 val loss 4.45175313949585 val acc 0.05613011494278908 remaining_tries 9\n",
      "epoch 193 train loss 6.677143812179565 val loss 4.451808452606201 val acc 0.05613011494278908 remaining_tries 8\n",
      "epoch 194 train loss 6.677138328552246 val loss 4.451752662658691 val acc 0.05613011494278908 remaining_tries 7\n",
      "epoch 195 train loss 6.677138566970825 val loss 4.451759576797485 val acc 0.05613011494278908 remaining_tries 6\n",
      "epoch 196 train loss 6.677138328552246 val loss 4.451762914657593 val acc 0.05613011494278908 remaining_tries 5\n",
      "epoch 197 train loss 6.677139043807983 val loss 4.451755523681641 val acc 0.05613011494278908 remaining_tries 4\n",
      "epoch 198 train loss 6.677139759063721 val loss 4.451756954193115 val acc 0.05613011494278908 remaining_tries 3\n",
      "epoch 199 train loss 6.677139520645142 val loss 4.451763868331909 val acc 0.05613011494278908 remaining_tries 2\n",
      "epoch 200 train loss 6.677140951156616 val loss 4.4517529010772705 val acc 0.05613011494278908 remaining_tries 1\n",
      "epoch 201 train loss 6.677145719528198 val loss 4.451752662658691 val acc 0.05613011494278908 remaining_tries 0\n",
      "epoch 202 train loss 6.6771392822265625 val loss 4.45176100730896 val acc 0.05613011494278908 remaining_tries -1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "C3AutoEncoder(\n",
       "  (encoder): C3Encoder(\n",
       "    (groups): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (1): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool3d(kernel_size=(2, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool3d(kernel_size=(2, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): MaxPool3d(kernel_size=(2, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_layers): ModuleList(\n",
       "      (0): Linear(in_features=316736, out_features=128, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): C3Decoder(\n",
       "    (fc_layers): ModuleList(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (final_linear): Linear(in_features=128, out_features=326144, bias=True)\n",
       "    (conv1d): Conv1d(1, 22, kernel_size=(1,), stride=(1,))\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchio as tio\n",
    "\n",
    "class NumpyImageDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self,x,y, transforms = True):\n",
    "        self.data = torch.from_numpy(x).float()\n",
    "        self.targets = torch.from_numpy(y).long()\n",
    "        if transforms:\n",
    "            transforms = tio.Compose([tio.RandomNoise(p=.5),tio.RandomAffine(p=.5)])\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "        \n",
    "        if self.transforms:\n",
    "            x = self.transforms(x)\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def train_torch(plist,model=None,img_size=56,batch_size=1,epochs=1000,patience=10):\n",
    "    torch.cuda.empty_cache()\n",
    "    x,y = arrays_from_plist(plist,add_channel=True,img_size=img_size)\n",
    "    nclasses = len(plist[0]['roi_mask_map'].keys()) + 1\n",
    "    if model is None:\n",
    "        model = C3AutoEncoder(x[0,0],num_classes=nclasses)\n",
    "    model = model.cuda()\n",
    "    \n",
    "    ce_loss = nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=.001)\n",
    "    \n",
    "    def get_loss(ypred,ytrain):\n",
    "        ytrain = ytrain.view(-1)\n",
    "        ypred = torch.moveaxis(ypred,1,-1).view(ytrain.shape[0],nclasses)\n",
    "        loss = ce_loss(ypred.float(),ytrain.long())\n",
    "        return loss\n",
    "    \n",
    "    def get_tp(ypred,y):\n",
    "        y = y.view(-1)\n",
    "        ypred = torch.moveaxis(ypred,1,-1).view(y.shape[0],nclasses)\n",
    "        ypred = torch.argmin(ypred,dim=1)\n",
    "        correct = torch.sum(ypred == y)\n",
    "        return correct/len(y)\n",
    "    \n",
    "    def get_loader(xx,yy,train):\n",
    "        dataset = NumpyImageDataset(xx, yy,transforms=train)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size = batch_size,shuffle=train)\n",
    "        return dataloader\n",
    "    \n",
    "    train_dataloader = get_loader(x[2:],y[2:],True)\n",
    "    val_dataloader = get_loader(x[:2],y[:2],True)\n",
    "    \n",
    "    \n",
    "    def train_step(xtrain,ytrain):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        xtrain = xtrain.cuda()\n",
    "        ytrain = ytrain.cuda()\n",
    "        ypred = model(xtrain)\n",
    "        loss = get_loss(ypred,ytrain)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss\n",
    "    \n",
    "    def val_step(xval,yval):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            xval = xval.cuda()\n",
    "            yval = yval.cuda()\n",
    "            ypred = model(xval)\n",
    "            loss = get_loss(ypred,yval)\n",
    "            true_positives = get_tp(ypred,yval)\n",
    "            return loss, true_positives\n",
    "        \n",
    "    best_val_loss = 100000000\n",
    "    steps_since_improvement = 0\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for xtrain, ytrain in train_dataloader:\n",
    "            loss = train_step(xtrain,ytrain)\n",
    "            epoch_loss  += loss.item()\n",
    "            \n",
    "        epoch_val_loss = 0\n",
    "        val_accuracy = 0\n",
    "        for xval, yval in val_dataloader:\n",
    "            loss,val_tp = val_step(xval,yval)\n",
    "            epoch_val_loss += loss.item()\n",
    "            val_accuracy += val_tp.item()\n",
    "        val_accuracy = val_accuracy/len(val_dataloader)\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            steps_since_improvement = 0\n",
    "        else:\n",
    "            steps_since_improvement += 1\n",
    "        print('epoch',epoch,'train loss',epoch_loss, 'val loss',epoch_val_loss,'val acc',val_accuracy,'remaining_tries',patience - steps_since_improvement)\n",
    "        if steps_since_improvement > patience:\n",
    "            break\n",
    "    return model\n",
    "\n",
    "train_torch(plist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c6a5944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d4a4d77-77ab-47df-b958-511deef9f73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3044f8af-48f7-4219-a6a7-07093ff90722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Qubbed]",
   "language": "python",
   "name": "conda-env-Qubbed-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
